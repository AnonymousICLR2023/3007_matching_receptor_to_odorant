{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import os\n",
    "import json\n",
    "from natsort import natsorted, ns\n",
    "import re \n",
    "import itertools\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "import pubchempy\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "# RDkit\n",
    "import rdkit\n",
    "from rdkit import Chem, DataStructs\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def get_family(x):\n",
    "    res = re.search(r'^OR([0-9]+)[A-Z]+[0-9]+$',x)[1]\n",
    "    return res\n",
    "\n",
    "def compute_smiles(cids_list, batch_size, properties='smiles'):\n",
    "    def batch(iterable, n=1):\n",
    "        l = len(iterable)\n",
    "        for ndx in range(0, l, n):\n",
    "            yield iterable[ndx:min(ndx + n, l)]\n",
    "    \n",
    "    _smiles = []\n",
    "    for x in batch(range(0, len(cids_list)), batch_size):\n",
    "        batch_cid = [cids_list[i] for i in x]\n",
    "        if properties == 'smiles':\n",
    "            batch_smiles = [i.isomeric_smiles for i in pubchempy.get_compounds(batch_cid, namespace='inchikey')]\n",
    "        elif properties == 'inchikey':\n",
    "            batch_smiles = [i.inchikey for i in pubchempy.get_compounds(batch_cid)]\n",
    "        _smiles.append(batch_smiles)\n",
    "    \n",
    "    return list(itertools.chain(*_smiles))\n",
    "\n",
    "def _change_units(x, _from, _to):\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except:\n",
    "        return x\n",
    "    if _from == 'uM' and _to == 'Log(M)':\n",
    "        return np.log10(x) - 6.0 # uM = 10^-6 M\n",
    "    elif _from == 'Log(M)' and _to == 'uM':\n",
    "        return 10.0**(x + 6.0)\n",
    "    elif _from == 'mM' and _to == 'uM':\n",
    "        return x*(10.0**3)\n",
    "\n",
    "def _func_data_ec50(data_ec50):\n",
    "    pairs_ec50 = data_ec50.groupby(['_MolID', 'mutated_Sequence']).apply(lambda x: x['Responsive'].mean()) # TODO: apply more complex function returning {'Responsive', 'sample_weight'}\n",
    "    if ((pairs_ec50 > 0.0)&(pairs_ec50 < 1.0)).any():\n",
    "        _ec50_inconsitent_idx = pairs_ec50[((pairs_ec50 > 0.0)&(pairs_ec50 < 1.0))].index\n",
    "        print('INFO: To discard EC50 inconsistent: {}'.format(len(_ec50_inconsitent_idx)))\n",
    "        pairs_ec50 = pairs_ec50.loc[pairs_ec50.index.difference(_ec50_inconsitent_idx)]\n",
    "    pairs_ec50 = pairs_ec50.astype(int)\n",
    "    pairs_ec50.name = 'Responsive'\n",
    "    pairs_ec50 = pairs_ec50.to_frame()\n",
    "    pairs_ec50['_DataQuality'] = 'ec50'\n",
    "    return pairs_ec50\n",
    "\n",
    "def _func_data_screening(data_screening):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def num_unique_value_screen(_df):\n",
    "        return len(_df['Value_Screen'].unique())\n",
    "\n",
    "    def _is_sorted(s):\n",
    "        return all(s.iloc[i] <= s.iloc[i+1] for i in range(len(s) - 1))\n",
    "        \n",
    "    _screening_consistency_ordering = data_screening.groupby(['_MolID', 'mutated_Sequence']).apply(lambda x: _is_sorted(x.sort_values(['Value_Screen', 'Responsive'])['Responsive']))\n",
    "    _screening_consistency_ordering.name = 'Check'\n",
    "    _screening_inconsistent_ordering_idx = _screening_consistency_ordering[~_screening_consistency_ordering].index\n",
    "    _screening_consistent_ordering_idx = _screening_consistency_ordering[_screening_consistency_ordering].index\n",
    "    print('INFO: To discard because of screening ordering (Inconsistent through Value_Screen): {}'.format(len(_screening_inconsistent_ordering_idx)))\n",
    "\n",
    "    _screening_consistency_per_value = data_screening.groupby(['_MolID', 'mutated_Sequence', 'Value_Screen']).apply(lambda x: ((x['Responsive']==1).all() or (x['Responsive']==0).all()))\n",
    "    _screening_consistency_per_value.name = 'Check'\n",
    "    _screening_consistency_per_value = _screening_consistency_per_value.reset_index('Value_Screen')\n",
    "    _screening_inconsitent_per_value_idx = _screening_consistency_per_value[~_screening_consistency_per_value['Check']].index\n",
    "    _screening_inconsitent_per_value_idx = _screening_inconsitent_per_value_idx.drop_duplicates()\n",
    "    _screening_consitent_per_value_idx = _screening_consistency_per_value[_screening_consistency_per_value['Check']].index\n",
    "    _screening_consitent_per_value_idx = _screening_consitent_per_value_idx.drop_duplicates()\n",
    "    print('INFO: To discard screening inconsistent per Value_Screen: {}'.format(len(_screening_inconsitent_per_value_idx)))\n",
    "\n",
    "    _screening_inconsitent_idx = _screening_inconsitent_per_value_idx.union(_screening_inconsistent_ordering_idx)\n",
    "    print('INFO: To discard screening inconsistent: {}'.format(len(_screening_inconsitent_idx)))\n",
    "\n",
    "    pairs_screening = data_screening.groupby(['_MolID', 'mutated_Sequence']).apply(lambda x: (x['Responsive']==1).any()).astype(int) # TODO: apply more complex function returning {'Responsive', 'sample_weight'}\n",
    "    pairs_screening.name = 'Responsive'\n",
    "    pairs_screening = pairs_screening.to_frame()\n",
    "    pairs_screening = pairs_screening.loc[pairs_screening.index.difference(_screening_inconsitent_idx)] # TODO: Discarding inconsistent here. Do we want to do it?\n",
    "\n",
    "    _count_unique_concentrations = data_screening.groupby(['_MolID', 'mutated_Sequence']).apply(num_unique_value_screen)\n",
    "    _count_unique_concentrations.name = 'num_unique_value_screen'\n",
    "\n",
    "    pairs_screening = pairs_screening.join(_count_unique_concentrations, how='left')        \n",
    "\n",
    "    pairs_primary = pairs_screening[pairs_screening['num_unique_value_screen'] == 1].copy()\n",
    "    pairs_primary['_DataQuality'] = 'primaryScreening'\n",
    "    pairs_secondary = pairs_screening[pairs_screening['num_unique_value_screen'] > 1].copy()\n",
    "    pairs_secondary['_DataQuality'] = 'secondaryScreening'\n",
    "\n",
    "    return pairs_primary, pairs_secondary\n",
    "\n",
    "def choose_smiles(x):\n",
    "    \"\"\"\n",
    "    This is consistent with prepro.py\n",
    "    \"\"\"\n",
    "    if x['GS_SMILES'] == x['GS_SMILES']:\n",
    "        return x['GS_SMILES']\n",
    "    else:\n",
    "        return x['paper_SMILES']\n",
    "\n",
    "def broadness_measure(x):\n",
    "    n = x.shape[0]\n",
    "    x_bool = x > 0.5\n",
    "    # return x_bool\n",
    "    return sum(x_bool).astype(np.float32) / n\n",
    "\n",
    "def get_map_CID_to_inchikey(cid):\n",
    "    mols = pubchempy.get_compounds(cid, 'cid')\n",
    "    return {mol.cid: mol.inchikey for mol in mols if mol is not None}\n",
    "\n",
    "def _unique_atomic_num(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles.strip())\n",
    "    X = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        X.append(atom.GetAtomicNum())\n",
    "    x = np.array(X, dtype = int)\n",
    "    return np.unique(x)\n",
    "    \n",
    "def _test_CNOS_atomic_num(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles.strip())\n",
    "    X = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        X.append(atom.GetAtomicNum())\n",
    "    x = np.array(X, dtype = int)\n",
    "    for ele in np.unique(x):\n",
    "        if ele not in [6, 7, 8, 16]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _get_kernel_density_contour_plot(samples, _xgrid, _ygrid):\n",
    "    X, Y = np.meshgrid(_xgrid, _ygrid)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    kde = KernelDensity(kernel=\"cosine\")\n",
    "    kde.fit(samples)\n",
    "\n",
    "    Z = np.exp(kde.score_samples(xy))\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    start_level = 0.0013 + Z.min() \n",
    "    end_level = Z.max()\n",
    "    levels = np.linspace(start_level, end_level, 10)\n",
    "\n",
    "    return X, Y, Z, levels, kde\n",
    "\n",
    "def _get_most_occuring_pyrfume_descriptor(df, i = 0, return_stats = False, top_k = 1):\n",
    "    s = df['pyrfume_odor'].str.split(',').explode()\n",
    "    _count = s.groupby(s).count().sort_values(ascending = False)\n",
    "    if return_stats:\n",
    "        return _count.index[i], _count.iloc[i], _count.iloc[i]/len(df)\n",
    "    else:\n",
    "        return '_'.join(_count.index[i:i+top_k].to_list())\n",
    "\n",
    "def get_quantile_per_odor_group(df, q = 0.75):\n",
    "    if len(df) > 9:\n",
    "        _X = np.stack(df['predict'])\n",
    "        _X = (_X > 0.5).astype(float)\n",
    "        _dist = pairwise_distances(_X, metric = 'l1')\n",
    "        try:\n",
    "            _dist_triu = _dist[np.triu_indices(n = _dist.shape[0], k = 1)] # /(len(df) - 1)\n",
    "            median_distances = np.median(_dist, axis=1)\n",
    "            # nanmedian_distances = np.nanmedian(_dist, axis=1)\n",
    "            # nanmedian_distances = np.sum(nanmedian_distances, axis=1) / (len(nanmedian_distances) - 1)  \n",
    "            return np.quantile(median_distances, q = q)\n",
    "        except Exception as e:\n",
    "            print(df['predict'])\n",
    "            raise e\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "def perform_mutation(row, mutation_col = 'Mutation', seq_col = 'Sequence'):\n",
    "    \"\"\"\n",
    "    Perform mutation on a row in pandas.DataFrame. This function is designed to be used in pandas apply.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    row : pandas.Series\n",
    "        row of pandas dataframe\n",
    "\n",
    "    mutation_col : str\n",
    "        name of the column with mutation information.\n",
    "\n",
    "    seq_col : str\n",
    "        name of the column with sequences.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    mutated_seq : str\n",
    "        mutated sequence\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if row[mutation_col] == row[mutation_col] and row[seq_col] == row[seq_col]:\n",
    "            seq = row[seq_col]\n",
    "            mutations = row[mutation_col].strip().split('_')\n",
    "            return _perform_mutation(mutations, seq)\n",
    "        else:\n",
    "            return row[seq_col]\n",
    "    except Exception as e:\n",
    "        print(row)\n",
    "        print(row[seq_col])\n",
    "        raise  e\n",
    "\n",
    "def _perform_mutation(mutations, seq):\n",
    "    \"\"\"\n",
    "    Main logic for performing mutations.\n",
    "    \"\"\"    \n",
    "    for mutation in mutations:\n",
    "        # print('->' + mutation)\n",
    "        _from = mutation[0]\n",
    "        _to = mutation[-1]\n",
    "        _position = int(mutation[1:-1]) - 1\n",
    "        if seq[_position] == _from:\n",
    "            seq = seq[:_position] + _to + seq[_position+1:]\n",
    "        else:\n",
    "            left_pos = _position - 5\n",
    "            right_pos = _position + 4\n",
    "            if _position < 5: left_pos = 0\n",
    "            if _position > len(seq) - 4: right_pos = len(seq)\n",
    "            raise 'Expected letter {} on position {} in sequence arround: {}. Found {}. Mutation: {}'.format(_from, _position, seq[left_pos:right_pos], seq[_position], mutation)\n",
    "    return seq\n",
    "\n",
    "def get_quantile_per_code_group(df, q = 0.75):\n",
    "    if len(df) > 15:\n",
    "        _X = np.stack(df['embed'])\n",
    "        _dist = pairwise_distances(_X, metric = 'l2')\n",
    "        try:\n",
    "            _dist_triu = _dist[np.triu_indices(n = _dist.shape[0], k = 1)] # /(len(df) - 1)\n",
    "            return np.quantile(_dist_triu, q = q)\n",
    "        except Exception as e:\n",
    "            print(df['embed'])\n",
    "            raise e\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "def merge_cols_with_priority(row, primary_col = 'Uniprot_Sequence', secondary_col = 'Sequence'): # merge_seqs\n",
    "    \"\"\"\n",
    "    merge two columns. primary column is kept and only if entry is missing use secondary_col.\n",
    "    This should be used in pandas apply.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    row : pandas.Series\n",
    "        row of pandas dataframe.\n",
    "    \n",
    "    primary_col : str\n",
    "        name of the main column.\n",
    "\n",
    "    secondary_col : str\n",
    "        name of the secondary column used only when info in the first is missing.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    entry : Any\n",
    "        value to put to the merged column.\n",
    "    \"\"\"\n",
    "    if row[primary_col] == row[primary_col]:\n",
    "        return row[primary_col]\n",
    "    else:\n",
    "        return row[secondary_col]\n",
    "\n",
    "def nDscatterPlot_online(ax, X, labels, special_labels, special_formatting, legend, cmap = None):\n",
    "    \"\"\"\n",
    "    Paramters:\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        array with (x, y) coordinates. It has to have shape [n_points, 2].\n",
    "\n",
    "    labels : numpy.ndarray\n",
    "        array with labels for each datapoint. It has to have the same first dimension as X.\n",
    "\n",
    "    special_labels : list\n",
    "        list of special labels, that can be annotated and have non-standard marker and properties.\n",
    "\n",
    "    special_formatting : dict\n",
    "        formatting of the special points. It is a dictionary with special_labels as keys and\n",
    "        values are dictionaries for each label's formatting. Structure is as follows:\n",
    "            {label_1 : {'marker' : 's',\n",
    "                        'annotations' : some_array,\n",
    "                        'color' : 0},\n",
    "             label_2 : {'marker' : 'x'},\n",
    "             label_3 : {'annotations' : some_array}}\n",
    "        If there is some missing property, default is used for it.\n",
    "        Currently availabel properties: ['marker', 'annotations']\n",
    "    \"\"\"\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.nipy_spectral\n",
    "    # cmap = plt.get_cmap('hsv')\n",
    "    if labels is None:\n",
    "        labels = np.zeros(X.shape[0], dtype=np.int32)\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    N = labels_unique.shape[0]\n",
    "    if N == 1:\n",
    "        N_eff = 1\n",
    "    else:\n",
    "        N_eff = N-1\n",
    "    for i, lab_i in enumerate(labels_unique):\n",
    "        c = cmap(float(i)/N_eff)\n",
    "        idx = np.squeeze(np.argwhere(labels==lab_i))\n",
    "        _X = [X[idx,i] for i in range(X.shape[1])]\n",
    "        if lab_i in special_labels:\n",
    "            formatting = special_formatting.get(lab_i, {'marker' : 'x',\n",
    "                                                        'annotations': [],\n",
    "                                                        'color' : 0,\n",
    "                                                        'size' : 6,\n",
    "                                                        'lw': 0,\n",
    "                                                        'lc':[(0,0,0,1)]})\n",
    "            marker = formatting.get('marker', 'x')\n",
    "            annos = formatting.get('annotations', [])\n",
    "            cmap_idx = formatting.get('color', 0)\n",
    "            size_coef = formatting.get('size', 6)\n",
    "            lw = formatting.get('lw', 0)\n",
    "            lc = formatting.get('lc', [(0,0,0,1)])\n",
    "            if len(idx.shape) > 0:\n",
    "                assert len(annos) == idx.shape[0] or len(annos)==0\n",
    "            else:\n",
    "                assert len(annos)==0\n",
    "            ax.scatter(*_X, color=cmap(cmap_idx), label=lab_i, marker=marker, s=size_coef*(plt.rcParams['lines.markersize'])**2, linewidths=lw, edgecolors=lc)\n",
    "            for _i, txt in enumerate(annos):\n",
    "                ax.text(*[_X[k].item(_i) + 0.3 for k in range(X.shape[1])], txt)\n",
    "        else:\n",
    "            ax.scatter(*_X, color=c, label=lab_i)\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.set_xlim((xlim[0], xlim[1]+3))\n",
    "    if legend:\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "def ORs_in_contour_plot(ax, _df_embed_joint, _embedding_name, _gene_names):\n",
    "    _X_all = np.stack(_df_embed_joint[_embedding_name])\n",
    "    _x_min, _y_min = np.min(_X_all, axis = 0)\n",
    "    _x_max, _y_max = np.max(_X_all, axis = 0)\n",
    "    _xgrid = np.arange(start = _x_min - 1, stop = _x_max + 1, step = 0.2)\n",
    "    _ygrid = np.arange(start = _y_min - 1, stop = _y_max + 1, step = 0.2)\n",
    "\n",
    "    _contour_plots = {}\n",
    "    for i, name in enumerate(_gene_names):\n",
    "        _tmp = df_predict_unstack[name]\n",
    "        _idx = _tmp[_tmp > 0.5].index.intersection(_df_embed_joint.index)\n",
    "        _s = _df_embed_joint.loc[_idx][_embedding_name]\n",
    "\n",
    "        _X = np.stack(_s)\n",
    "        X, Y, Z, levels, _  = _get_kernel_density_contour_plot(_X, _xgrid, _ygrid)\n",
    "\n",
    "        #_cmap = plt.cm.get_cmap('Blues')\n",
    "        _cmap = plt.cm.get_cmap('viridis')\n",
    "        newcolors = np.array(_cmap(np.linspace(0, 1, 256))) + np.array([[100*i/256, 3*i/256, 3*i/256, 0]])\n",
    "        newcolors = list((newcolors - np.min(newcolors, axis = 0))/np.max(newcolors, axis = 0))\n",
    "        newcmp = ListedColormap(newcolors)\n",
    "\n",
    "        def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=256):\n",
    "            new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "                'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "                cmap(np.linspace(minval, maxval, n)))  \n",
    "            return new_cmap\n",
    "\n",
    "        greys = truncate_colormap(plt.cm.get_cmap('Greys'),0.4,1)\n",
    "        reds = truncate_colormap(plt.cm.get_cmap('Greens'),0.4,1)\n",
    "        blues = truncate_colormap(plt.cm.get_cmap('Blues'),0.4,1)\n",
    "        colormaplist = [greys, reds, blues]\n",
    "        newcmp = colormaplist[i]\n",
    "\n",
    "        _contour_plots[name] = ax.contourf(X, Y, Z, levels=levels, cmap=newcmp, alpha = 0.45)\n",
    "        _pos_max = np.where(Z == np.max(Z))\n",
    "\n",
    "        _dummy_point = ax.plot(X[_pos_max], Y[_pos_max], label = name, color=newcolors[0][:3]) # Create a dummy point\n",
    "\n",
    "        del _tmp, _idx, _s, _X, newcmp, newcolors\n",
    "\n",
    "    \n",
    "    def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=256):\n",
    "        new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "            'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "            cmap(np.linspace(minval, maxval, n)))  \n",
    "        return new_cmap\n",
    "    a = truncate_colormap(plt.get_cmap('copper'),.5,1)(np.linspace(0., 1, 128))\n",
    "    b = truncate_colormap(plt.get_cmap('jet'),0.8,1)(np.linspace(0., 1, 128))\n",
    "    couleur = np.vstack((a, b,[0., 0., 0, 1.]))\n",
    "    mymap = LinearSegmentedColormap.from_list('my_colormap', couleur)\n",
    "\n",
    "    # plots.nDscatterPlot(ax, X = _X_all, labels = np.stack(_df_embed_joint['GS_Odor type']), special_labels = [], special_formatting = {}, legend = True, cmap = None)\n",
    "    a = nDscatterPlot_online(ax, X = _X_all, labels = np.stack(_df_embed_joint['embedding_cluster_top_descriptor']), \n",
    "                                                                special_labels = ['Nutty $1^{st}$', 'Nutty $2^{nd}$', 'Nutty $3^{rd}$', 'Woody $1^{st}$', 'Woody $2^{nd}$','Woody $3^{rd}$', '_'], \n",
    "                                                                special_formatting = {'Nutty $1^{st}$' : {'marker' : 'o', 'annotations': [], 'color' : 126, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        'Nutty $2^{nd}$' : {'marker' : 'o', 'annotations': [], 'color' : 50, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        'Nutty $3^{rd}$' : {'marker' : 'o', 'annotations': [], 'color' : 0, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        #'nutty_4' : {'marker' : 'o', 'annotations': [], 'color' : 160, 'size' : 0.7},\n",
    "                                                                                        'Woody $1^{st}$' : {'marker' : 'o', 'annotations': [], 'color' : 130, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        'Woody $2^{nd}$' : {'marker' : 'o', 'annotations': [], 'color' : 190, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        'Woody $3^{rd}$' : {'marker' : 'o', 'annotations': [], 'color' : 220, 'size' : 1.3,'lw':0.5,'lc':(0,0,0,1)},\n",
    "                                                                                        #'woody_4' : {'marker' : 'o', 'annotations': [], 'color' : 210, 'size' : 0.7},\n",
    "                                                                                        '_' : {'marker' : 'o', 'annotations': [], 'color' : 258, 'size' : 0.3},\n",
    "                                                                }, legend = True, cmap = mymap)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35824/3150660313.py:2: DtypeWarning: Columns (2,5,10,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_full = pd.read_csv('../Dataset/export_20220512-135815.csv', sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: To discard EC50 inconsistent: 17\n",
      "INFO: To discard because of screening ordering (Inconsistent through Value_Screen): 144\n",
      "INFO: To discard screening inconsistent per Value_Screen: 53\n",
      "INFO: To discard screening inconsistent: 166\n"
     ]
    }
   ],
   "source": [
    "# df_full = pd.read_csv('./Data/export_20220512-135815.csv', sep=';')\n",
    "df_full = pd.read_csv('../Dataset/export_20220512-135815.csv', sep=';')\n",
    "# df_uniprot = pd.read_csv('./Data/uniprot_sequences.csv', sep = ';', index_col = None)\n",
    "df_uniprot = pd.read_csv('../Dataset/uniprot_sequences.csv', sep = ';', index_col = None)\n",
    "df = pd.read_csv('./Data/predict_Model20220613-091215_e10000.csv', sep=';')\n",
    "seq_input_df = pd.read_csv('./Data/seqs.csv', sep=';', index_col = None)\n",
    "mol_input_df = pd.read_csv('./Data/mols.csv', sep=';', index_col = None)\n",
    "seq_df_raw = pd.read_json('./Data/table_ncbi_identifier_name_sequence_stripped.json', orient='index')\n",
    "df_odor = pd.read_csv('./Data/merged_GoodScent_merged_paper_SMILES_to_Smell.csv', sep=';')\n",
    "df_odor_embedding_json = pd.read_json('./Data/pyrfume_embedding_2022_09_08_embed.json', orient = 'index').set_index('_SMILES')\n",
    "df_odor_embedding_index = pd.read_csv('./Data/pyrfume_embedding_2022_09_08_source.csv', sep=';', index_col=0)\n",
    "df_pyrfume = pd.read_csv('./Data/full_data.csv', sep=';', index_col = 0)\n",
    "with open('./Data/map_CID_to_inchikey.json', 'r') as jsonfile:\n",
    "    pyrfume_map_cid_to_inchikey = json.load(jsonfile)\n",
    "with open('./Data/classes_mapping.json', 'r') as jsonfile:\n",
    "    pyrfume_classes_mapping = json.load(jsonfile)\n",
    "    pyrfume_classes_mapping = {val: key for key, val in pyrfume_classes_mapping.items()}\n",
    "with open('./Data/map_inchikey_to_isomericSMILES.json', 'r') as jsonfile:\n",
    "    map_inchikey_to_isomericSMILES = json.load(jsonfile)\n",
    "\n",
    "\n",
    "df_full = df_full[df_full.Mixture != 'mixture']\n",
    "df_uniprot.set_index('Entry', drop = True, inplace = True)\n",
    "df_full = df_full.join(df_uniprot[['Uniprot_Sequence']], on = 'Uniprot ID', how = 'left')\n",
    "df_full['_MolID'] = df_full.apply(lambda x: merge_cols_with_priority(x, primary_col = 'InChI Key', secondary_col = 'canonicalSMILES'), axis = 1)\n",
    "df_full['_Sequence'] = df_full.apply(lambda x: merge_cols_with_priority(x, primary_col = 'Uniprot_Sequence', secondary_col = 'Sequence'), axis = 1)\n",
    "df_full['mutated_Sequence'] = df_full.apply(lambda x: perform_mutation(x, mutation_col = 'Mutation', seq_col = '_Sequence'), axis = 1)\n",
    "# Change units\n",
    "df_full.loc[df_full['Unit'] == 'uM', 'Value'] = df_full[df_full['Unit'] == 'uM']['Value'].apply(lambda x: _change_units(x, _from = 'uM', _to = 'Log(M)'))\n",
    "df_full.loc[df_full['Unit'] == 'uM', 'Unit'] = 'Log(M)'\n",
    "df_full.loc[df_full['Unit_Screen'] == 'mM', 'Value_Screen'] = df_full[df_full['Unit_Screen'] == 'mM']['Value_Screen'].apply(lambda x: _change_units(x, _from = 'mM', _to = 'uM'))\n",
    "df_full.loc[df_full['Unit_Screen'] == 'mM', 'Unit_Screen'] = 'uM'\n",
    "# process EC50\n",
    "data_ec50 = df_full[df_full['Parameter'] == 'ec50']\n",
    "pairs_ec50 =_func_data_ec50(data_ec50)\n",
    "# process Screening\n",
    "data_screening = df_full[df_full['Parameter'] != 'ec50']\n",
    "if not data_screening.empty:\n",
    "        pairs_primary, pairs_secondary = _func_data_screening(data_screening)\n",
    "        pairs_primary = pairs_primary.loc[pairs_primary.index.difference(pairs_ec50.index)] \n",
    "        pairs_secondary = pairs_secondary.loc[pairs_secondary.index.difference(pairs_ec50.index)]\n",
    "\n",
    "        pairs = pd.concat([pairs_ec50, pairs_primary, pairs_secondary])\n",
    "else:\n",
    "    pairs = pairs_ec50\n",
    "pairs['Responsive'] = pairs['Responsive'].astype(int)\n",
    "\n",
    "# Merge sequence and molecule information.\n",
    "_tmp_seq = pd.merge(seq_input_df, seq_df_raw, left_on = 'mutated_Sequence', right_on = 'Sequence')\n",
    "_tmp_seq = _tmp_seq[['seq_id', 'Accesion_Number','Gene','Sequence']]\n",
    "_tmp_pred = pd.merge(df, mol_input_df, on = '_SMILES')\n",
    "df_predict = pd.merge(_tmp_pred, _tmp_seq, on='seq_id')\n",
    "df_predict.set_index(['seq_id', 'mol_id'], drop=True, inplace = True)\n",
    "df_predict.rename(columns = {'pred':'predict', '_MolID' : 'InchiKey'}, inplace = True)\n",
    "\n",
    "df_predict_unstack = df_predict.copy()\n",
    "df_predict_unstack.set_index(['Gene', 'InchiKey'], drop = True, inplace = True)\n",
    "df_predict_unstack = df_predict_unstack['predict'].unstack(level = 0)\n",
    "\n",
    "df_odor['SMILES'] = df_odor.apply(choose_smiles, axis=1)\n",
    "df_odor.set_index('InchiKey', inplace = True)\n",
    "\n",
    "# odor embedding:\n",
    "df_odor_embedding = df_odor_embedding_index.join(df_odor_embedding_json, on = '_SMILES', how='inner')\n",
    "df_odor_embedding.rename(columns = {'prediction' : 'odorModel_label', 'InChI Key' : 'InchiKey', '_SMILES' :'SMILES'}, inplace = True)\n",
    "df_odor_embedding.drop(columns = ['_MolID', 'canonicalSMILES'], inplace = True)\n",
    "df_odor_embedding.set_index('InchiKey', inplace = True)\n",
    "\n",
    "# Map one-hot to labels:\n",
    "df_pyrfume['pyrfume_odor'] = df_pyrfume['Values'].apply(lambda x: ','.join([pyrfume_classes_mapping[key] for key in list(np.where(np.array(x[1:-1].split(', '), dtype=int) == 1)[0])]))\n",
    "df_pyrfume['InchiKey'] = df_pyrfume['CID'].astype(int).astype(str).map(pyrfume_map_cid_to_inchikey)\n",
    "\n",
    "# Add GoodScent descriptors: \n",
    "df_odor = df_pyrfume.join(df_odor, on = 'InchiKey', how = 'left', rsuffix = '_GS')\n",
    "\n",
    "# Mols view:\n",
    "df_mols = df_predict_unstack.apply(lambda x: np.array(x), axis = 1)\n",
    "df_mols.name = 'predict'\n",
    "df_mols = df_mols.to_frame()\n",
    "df_mols['SMILES'] = df_mols.index.map(map_inchikey_to_isomericSMILES)\n",
    "df_mols['broadness'] = df_mols['predict'].apply(broadness_measure)\n",
    "\n",
    "# OR view\n",
    "df_OR = df_predict.groupby('Gene').apply(lambda x: np.array(x['predict']))\n",
    "df_OR.name = 'predict'\n",
    "df_OR = df_OR.to_frame()\n",
    "df_OR['broadness'] = df_OR['predict'].apply(broadness_measure)\n",
    "\n",
    "\n",
    "_tmp = df_mols[df_mols['broadness'] <= 10e-7].copy()\n",
    "_tmp['max_predict'] = _tmp['predict'].apply(lambda x: np.max(x))\n",
    "# Save non active SMILES\n",
    "non_active_mols_idx = _tmp.index\n",
    "del _tmp\n",
    "\n",
    "\n",
    "df_mols_active = df_mols.loc[df_mols.index.difference(non_active_mols_idx)].copy()\n",
    "df_mols_non_active = df_mols.loc[non_active_mols_idx].copy()\n",
    "\n",
    "df_mols = df_mols_active.copy()\n",
    "df_mols = df_mols[df_mols['SMILES'].apply(_test_CNOS_atomic_num)].copy()\n",
    "df_odor = df_odor[~(df_odor['pyrfume_odor'].str.contains('odorless'))].copy()\n",
    "\n",
    "_odor = df_odor.copy()\n",
    "_odor = _odor.set_index('InchiKey')\n",
    "_odor = _odor['pyrfume_odor'].dropna() \n",
    "_odor = df_mols.join(_odor, how = 'inner')\n",
    "\n",
    "_odor_type = _odor.groupby('pyrfume_odor').count()['predict']\n",
    "_odor = _odor[_odor['pyrfume_odor'].isin(_odor_type.index)] ## Pyrfume\n",
    "\n",
    "# Generate clustering based on odor predcition embedding.\n",
    "_data = np.stack(df_odor_embedding['embed'], axis = 0)\n",
    "uamp_embedding = umap.UMAP(n_neighbors=15, min_dist=0.0, n_components=5, random_state=345).fit_transform(_data) # 15, 0\n",
    "labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=10).fit_predict(uamp_embedding) # 15 , 12 # 10, 10\n",
    "df_odor_embedding['uamp_embedding'] = list(uamp_embedding)\n",
    "df_odor_embedding['embedding_cluster_hdbscan'] = labels.astype(int)\n",
    "\n",
    "_odor = df_mols.join(df_odor_embedding['embedding_cluster_hdbscan'], on = 'InchiKey', how='left')\n",
    "\n",
    "_odor_type = _odor.groupby('embedding_cluster_hdbscan').count()['predict']\n",
    "_odor = _odor[_odor['embedding_cluster_hdbscan'].isin(_odor_type.index)]\n",
    "\n",
    "# Generate clustering based on odor predcition embedding.\n",
    "df_code = df_mols.copy()\n",
    "_data = np.stack(df_code['predict'], axis = 0)\n",
    "uamp_embedding = umap.UMAP(n_neighbors=15, min_dist=0.0, n_components=5, random_state=345).fit_transform(_data) # 15, 0\n",
    "labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=10, metric = 'manhattan').fit_predict(uamp_embedding) # 15 , 12 # 10, 10\n",
    "df_code['code_uamp_embedding'] = list(uamp_embedding)\n",
    "df_code['code_cluster_hdbscan'] = labels.astype(int)\n",
    "labels = AgglomerativeClustering(n_clusters = 100).fit_predict(_data)\n",
    "df_code['code_cluster_aglo'] = labels.astype(int)\n",
    "\n",
    "df_mols_joint = df_mols.copy()\n",
    "df_mols_joint = df_mols_joint.join(df_odor_embedding[['embedding_cluster_hdbscan', 'odorModel_label', 'embed', 'uamp_embedding']], on = 'InchiKey', how='inner')\n",
    "df_mols_joint = pd.merge(df_mols_joint, df_odor[['pyrfume_odor', 'GS_Odor type', 'merged_odors', 'InchiKey']], on = 'InchiKey', how='inner')\n",
    "df_mols_joint = df_mols_joint.join(df_code[['code_cluster_hdbscan', 'code_uamp_embedding', 'code_cluster_aglo']], on = 'InchiKey', how='inner')\n",
    "df_mols_joint.set_index('InchiKey', inplace = True)\n",
    "\n",
    "embedding_cluster_top_descriptor_mapping = df_mols_joint.groupby('embedding_cluster_hdbscan').apply(lambda x: _get_most_occuring_pyrfume_descriptor(x, i=0, return_stats=False, top_k = 3)).to_dict()\n",
    "embedding_cluster_top_descriptor_mapping.update({-1 : 'unclustered'})\n",
    "df_mols_joint['embedding_cluster_top_descriptor'] = df_mols_joint['embedding_cluster_hdbscan'].map(embedding_cluster_top_descriptor_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Quantile Distribution (Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_df = df_mols_joint.copy()\n",
    "_df = _df[_df['embedding_cluster_hdbscan'] >= 0]\n",
    "_temp=pd.DataFrame()\n",
    "_temp['100%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 1,)\n",
    "_temp['90%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.9,)\n",
    "_temp['75%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.75,)\n",
    "_temp['50%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.5,)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.histplot(data=_temp.melt(),x='value', hue='variable', cumulative=True, fill=False,\n",
    "             bins=10000, alpha=1, element='step',palette='tab10', linewidth=2.5, stat='density', common_norm=False)\n",
    "plt.xlabel(r\"$Q_{k}(\\alpha)$\", labelpad=25)\n",
    "plt.ylabel('Percentage of clusters',labelpad=25)\n",
    "plt.xlim([0,350])\n",
    "plt.xticks([0,50,100,150,200,250,300,350])\n",
    "plt.legend(['50%','75%', '90%', '100%'],loc='lower right', title=r'$\\alpha$', fontsize=26,title_fontsize=26)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(f'./Figures/Quantile_Distribution_Cumulative_{date.today()}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_df = df_mols_joint.copy()\n",
    "_df = _df[_df['embedding_cluster_hdbscan'] >= 0]\n",
    "_temp=pd.DataFrame()\n",
    "_temp['100%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 1,)\n",
    "_temp['90%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.9,)\n",
    "_temp['75%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.75,)\n",
    "_temp['50%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.5,)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.kdeplot(data=_temp.melt(),x='value', hue='variable',common_norm=False,\n",
    "            alpha=1, palette='tab10', fill=False, bw_method='scott', bw_adjust=0.6, lw=3)\n",
    "plt.xlabel(r\"$Q_{k}(\\alpha)$\", labelpad=25)\n",
    "plt.ylabel('Density', labelpad=25)\n",
    "plt.xlim([0,350])\n",
    "plt.xticks([0,50,100,150,200,250,300,350])\n",
    "plt.legend(['50%','75%', '90%', '100%'],loc='upper right', title=r'$\\alpha$',fontsize=26,title_fontsize=26)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(f'./Figures/Quantile_Distribution_{date.today()}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Broadness Distribution (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 30\n",
    "\n",
    "df_predict['Responsive'] = df_predict.predict > 0.5\n",
    "df_predict.reset_index(inplace=True)\n",
    "\n",
    "_temp_dfs = []\n",
    "for seq in df_predict.seq_id.unique():\n",
    "    _temp_dfs.append(df_predict[(df_predict.seq_id == seq) & (df_predict.Responsive == 1)])\n",
    "\n",
    "import tanimoto_similarity\n",
    "pool = multiprocessing.Pool(processes=20)\n",
    "\n",
    "mean_tanimoto = pool.map(tanimoto_similarity.get_tanimoto_matrix, _temp_dfs)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "broadness_df = df_predict.groupby('seq_id').apply(lambda x: x.Responsive.sum() / x.Responsive.count()).reset_index().rename(columns={0:'Broadness'})\n",
    "broadness_df = broadness_df.merge(df_predict.drop_duplicates(subset='seq_id')[['seq_id','Sequence','Gene']], on='seq_id')\n",
    "broadness_df = broadness_df[['Sequence','seq_id','Broadness','Gene']]\n",
    "\n",
    "order_dict = pd.Series(natsorted(broadness_df.Gene, alg=ns.IGNORECASE)).to_dict()\n",
    "order_dict2 = {y: x for x, y in order_dict.items()}\n",
    "broadness_df['natural order'] = broadness_df.Gene.map(order_dict2)\n",
    "broadness_df['Tanimoto'] = mean_tanimoto\n",
    "\n",
    "_broadness_df = broadness_df.sort_values('natural order')\n",
    "_broadness_df['family'] = _broadness_df.Gene.apply(lambda x: get_family(x))\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(25,10),gridspec_kw={'height_ratios': [15, 1]})\n",
    "fig.subplots_adjust(hspace=0.1)\n",
    "\n",
    "copper = plt.get_cmap('copper_r')\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "copper = truncate_colormap(copper, 0.1, 1)\n",
    "\n",
    "rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "ax1.bar(_broadness_df['natural order'], height=_broadness_df['Broadness'], width=1., color=copper(rescale(_broadness_df.Tanimoto)))\n",
    "a = mpl.cm.tab20\n",
    "\n",
    "for i in broadness_df.sort_values('Broadness', ascending=False).head(10).index:\n",
    "    ax1.text(_broadness_df.loc[i]['natural order'], _broadness_df.loc[i]['Broadness'], _broadness_df.loc[i]['Gene'], fontsize=19, rotation=70)\n",
    "\n",
    "for i, fam in enumerate(_broadness_df.family.unique()):\n",
    "    bar=ax2.barh(y = 0, left=_broadness_df[_broadness_df.family == fam]['natural order'].min(), width=len(_broadness_df[_broadness_df.family == fam]['natural order']), color=a.colors[i], height=0.5, align='center')\n",
    "    if fam == '12' or fam == '3' :\n",
    "        lab = ['']\n",
    "    else:\n",
    "        lab = [str(fam)]\n",
    "    ax2.bar_label(bar, labels=lab, label_type='center',padding=0.0, fontsize=19,fontweight='bold')\n",
    "\n",
    "plot = plt.scatter(_broadness_df['natural order'], _broadness_df['Broadness'], c = _broadness_df['Tanimoto'], cmap=copper, s=0)\n",
    "cbaxes = inset_axes(ax1, width=\"30%\", height=\"3%\", loc=1)\n",
    "plt.colorbar(plot, cax=cbaxes, orientation='horizontal',ticks=[0.1,0.15,0.2,0.25])\n",
    "plt.xlabel('Mean Tanimoto similarity')\n",
    "\n",
    "\n",
    "ax1.set_xticks([],[])\n",
    "ax2.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax2.spines.left.set_visible(False)\n",
    "ax2.spines.right.set_visible(False)\n",
    "ax2.set_yticks([],[])\n",
    "ax1.set_ylim([0,0.65])\n",
    "ax2.set_xlabel('Olfactory receptors', labelpad=25)\n",
    "ax1.set_ylabel('Broadness', labelpad=25)\n",
    "plt.savefig(f'./Figures/Broadness_Distribution_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Combinatorial Code Paper (Figure 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_order = df_OR.sort_values('broadness', ascending=False).index\n",
    "_df = df_mols_joint.copy()\n",
    "_df = _df[_df['embedding_cluster_hdbscan'] >= 0]\n",
    "_temp=pd.DataFrame()\n",
    "_temp['100%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 1,)\n",
    "_temp['90%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.9,)\n",
    "_temp['75%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.75,)\n",
    "_temp['50%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.5,)\n",
    "\n",
    "for clus in [137, 64, 174, 3]: # [167, 38, 188, 4]\n",
    "    cluster = clus\n",
    "    mat_cluster = df_predict_unstack.loc[df_mols_joint[df_mols_joint.embedding_cluster_hdbscan == cluster].sort_values('broadness', ascending=False).index, gene_order]\n",
    "    q50 = int(_temp.loc[cluster, '50%'])\n",
    "    q75 = int(_temp.loc[cluster, '75%'])\n",
    "    q90 = int(_temp.loc[cluster, '90%'])\n",
    "    q100 = int(_temp.loc[cluster, '100%'])\n",
    "    print(df_mols_joint[df_mols_joint.embedding_cluster_hdbscan == cluster].embedding_cluster_top_descriptor.unique())\n",
    "    print(f\"Q(a)({0.5, 0.75, 0.9, 1.0}) = {q50},{q75},{q90},{q100}\")\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.matshow(mat_cluster)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    #plt.savefig(f'./Figures/Combinatorial_Code_Cluster_{cluster}_{date.today()}.png',bbox_inches='tight',pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Umap Odor Embedding (Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clustering based on odor predcition embedding.\n",
    "_df_embed = df_odor_embedding.copy()\n",
    "_data = np.stack(_df_embed['embed'], axis = 0)\n",
    "uamp_embedding = umap.UMAP(n_neighbors=15, min_dist=0.5, n_components=2, random_state=345).fit_transform(_data)\n",
    "labels = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=10).fit_predict(uamp_embedding) # 15 , 12\n",
    "_df_embed['uamp_embedding'] = list(uamp_embedding)\n",
    "_df_embed['embedding_cluster_hdbscan'] = labels.astype(int)\n",
    "\n",
    "_df_embed = _df_embed.join(df_odor_embedding['embedding_cluster_hdbscan'], rsuffix = '_5D')\n",
    "\n",
    "_df_embed_joint = _df_embed.join(df_mols_joint[['pyrfume_odor']], how = 'inner')\n",
    "_df_embed_joint = _df_embed_joint.dropna(subset = ['pyrfume_odor'])\n",
    "_embedding_cluster_top_descriptor_mapping = _df_embed_joint.groupby('embedding_cluster_hdbscan_5D').apply(lambda x: _get_most_occuring_pyrfume_descriptor(x, i=0, return_stats=False, top_k = 3)).to_dict()\n",
    "_embedding_cluster_top_descriptor_mapping.update({-1 : '-1'})\n",
    "\n",
    "_res = {}\n",
    "j=0\n",
    "for key, val in _embedding_cluster_top_descriptor_mapping.items():\n",
    "    if len(val.split('_')) >1:\n",
    "        if ('nutty' == val.split('_')[0]) or ('nut'==val.split('_')[0]):\n",
    "            _res[key] = 'Nutty $1^{st}$'\n",
    "        elif ('nutty' == val.split('_')[1]) or ('nut'==val.split('_')[1]):\n",
    "            _res[key] = 'Nutty $2^{nd}$'\n",
    "        elif ('nutty' == val.split('_')[2]) or ('nut'==val.split('_')[2]):\n",
    "            _res[key] = 'Nutty $3^{rd}$'\n",
    "        elif 'woody' in val.split('_')[0]:\n",
    "            _res[key] = 'Woody $1^{st}$'\n",
    "        elif 'woody' in val.split('_')[1]:\n",
    "            _res[key] = 'Woody $2^{nd}$'\n",
    "        elif 'woody' in val.split('_')[2]:\n",
    "            _res[key] = 'Woody $3^{rd}$'\n",
    "        else:\n",
    "            _res[key] = '_'\n",
    "    else:\n",
    "        _res[key] = '_'\n",
    "_embedding_cluster_top_descriptor_mapping = _res\n",
    "\n",
    "\n",
    "_df_embed_joint['embedding_cluster_top_descriptor'] = _df_embed_joint['embedding_cluster_hdbscan_5D'].map(_embedding_cluster_top_descriptor_mapping)\n",
    "\n",
    "_embedding_name = 'uamp_embedding' # 'pca_embedding' 'uamp_embedding'\n",
    "plt.rcParams['font.size'] = 20\n",
    "fig = plt.figure(figsize = (25, 16))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_gene_names = ['OR2W1', 'OR10W1', 'OR5K1']\n",
    "ORs_in_contour_plot(ax, _df_embed_joint, _embedding_name, _gene_names)\n",
    "lgnd = ax.legend( prop={'size': 20})\n",
    "for i in range(3,9):\n",
    "    lgnd.legendHandles[i]._sizes = [200]\n",
    "    \n",
    "lgnd.legendHandles[0].set_linewidth(5)\n",
    "lgnd.legendHandles[0].set_color('grey')\n",
    "lgnd.legendHandles[1].set_linewidth(5)\n",
    "lgnd.legendHandles[1].set_color('green')\n",
    "lgnd.legendHandles[2].set_linewidth(5)\n",
    "lgnd.legendHandles[2].set_color('blue')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.savefig(f'./Figures/Receptor_Odor_Umap_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Olfactory Receptors vs Odors Distribution (Figure 6 - a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1 By Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_pairs = pairs.reset_index()\n",
    "non_responsive = pd.DataFrame(_pairs[_pairs.Responsive == 0].groupby(['_MolID']).count()['mutated_Sequence'])\n",
    "responsive = pd.DataFrame(_pairs[_pairs.Responsive == 1].groupby(['_MolID']).count()['mutated_Sequence'])\n",
    "repartition = non_responsive.merge(responsive, left_index=True, right_index=True,how='outer').replace(float('nan'),0)\n",
    "repartition['total'] = repartition['mutated_Sequence_x'] + repartition['mutated_Sequence_y'] \n",
    "\n",
    "_repartition = repartition.sort_values('total', ascending=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,10))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "ax1.bar(_repartition.index, height=_repartition.mutated_Sequence_y, width=1., color='tab:red',edgecolor='black',linewidth=0.0)\n",
    "ax1.bar(_repartition.index, bottom=_repartition.mutated_Sequence_y, height=_repartition.mutated_Sequence_x , width=1.,color='silver')\n",
    "\n",
    "ax2.bar(_repartition.index, bottom=_repartition.mutated_Sequence_y, height=_repartition.mutated_Sequence_x , width=1.,color='silver')\n",
    "ax2.bar(_repartition.index, height=_repartition.mutated_Sequence_y, width=1.,color='tab:red',edgecolor='black',linewidth=0.0)\n",
    "\n",
    "ax1.set_ylim(310, 800)\n",
    "ax2.set_ylim(0, 180)\n",
    "\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax1.set_xticks([])\n",
    "ax2.set_xticks([])\n",
    "ax1.legend(['Agonists', 'Non-Agonists'])\n",
    "\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax2.set_xlabel('Odorants', labelpad=25)\n",
    "ax1.set_ylabel('Number of ORs tested', y=0.0001, labelpad=25)\n",
    "plt.savefig(f'./Figures/Distribution_OR_Odor_by_activity_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 By Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_pairs = pairs.reset_index()\n",
    "ec50 = pd.DataFrame(_pairs[_pairs._DataQuality == 'ec50'].groupby(['_MolID']).count()['mutated_Sequence'])\n",
    "secondary = pd.DataFrame(_pairs[_pairs._DataQuality == 'secondaryScreening'].groupby(['_MolID']).count()['mutated_Sequence'])\n",
    "primary = pd.DataFrame(_pairs[_pairs._DataQuality == 'primaryScreening'].groupby(['_MolID']).count()['mutated_Sequence'])\n",
    "\n",
    "repartition = ec50.merge(secondary, left_index=True, right_index=True,how='outer').merge(primary, left_index=True, right_index=True,how='outer').replace(float('nan'),0)\n",
    "repartition['total'] = repartition['mutated_Sequence_x'] + repartition['mutated_Sequence_y'] + repartition['mutated_Sequence'] \n",
    "\n",
    "_repartition = repartition.sort_values('total', ascending=False)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,10))\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "\n",
    "ax1.bar(_repartition.index, height=_repartition.mutated_Sequence_x, width=1.5, )\n",
    "ax1.bar(_repartition.index, bottom=_repartition.mutated_Sequence_x, height=_repartition.mutated_Sequence_y , width=1.5, )\n",
    "ax1.bar(_repartition.index, bottom=_repartition.mutated_Sequence_y +_repartition.mutated_Sequence_x, height=_repartition.mutated_Sequence , width=1.5, color='silver')\n",
    "\n",
    "ax2.bar(_repartition.index, height=_repartition.mutated_Sequence_x, width=1.5, )\n",
    "ax2.bar(_repartition.index, bottom=_repartition.mutated_Sequence_x, height=_repartition.mutated_Sequence_y , width=1.5, )\n",
    "ax2.bar(_repartition.index, bottom=_repartition.mutated_Sequence_y +_repartition.mutated_Sequence_x, height=_repartition.mutated_Sequence , width=1.5,color='silver')\n",
    "\n",
    "ax1.set_ylim(310, 800)\n",
    "ax2.set_ylim(0, 180)\n",
    "\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.tick_params(labeltop=False)\n",
    "ax2.xaxis.tick_bottom()\n",
    "ax1.set_xticks([])\n",
    "ax2.set_xticks([])\n",
    "\n",
    "\n",
    "ax1.legend(['EC50','Secondary','Primary'])\n",
    "d = 1.5 \n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax2.set_xlabel('Odorants', labelpad=25)\n",
    "ax1.set_ylabel('Number of ORs tested', y=0.0001, labelpad=25)\n",
    "plt.savefig(f'./Figures/Distribution_OR_Odor_by_test_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Odors vs Olfactory Receptors Distribution (Figure 6 - c/d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 By Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_pairs = pairs.reset_index()\n",
    "non_responsive = pd.DataFrame(_pairs[_pairs.Responsive == 0].groupby(['mutated_Sequence']).count()['_MolID'])\n",
    "responsive = pd.DataFrame(_pairs[_pairs.Responsive == 1].groupby(['mutated_Sequence']).count()['_MolID'])\n",
    "repartition = non_responsive.merge(responsive, left_index=True, right_index=True,how='outer').replace(float('nan'),0)\n",
    "\n",
    "repartition['total'] = repartition['_MolID_x'] + repartition['_MolID_y'] \n",
    "\n",
    "_repartition = repartition.sort_values('total', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(_repartition.index, height=_repartition._MolID_y, width=1, color='tab:red')\n",
    "plt.bar(_repartition.index, bottom=_repartition._MolID_y, height=_repartition._MolID_x , width=1,color='silver')\n",
    "plt.xticks([],[])\n",
    "plt.legend(['Agonists','Non-Agonists'])\n",
    "plt.xlabel('Olfactory receptors', labelpad=25)\n",
    "plt.ylabel('Number of odorants tested',labelpad=25)\n",
    "plt.savefig(f'./Figures/Distribution_Odor_OR_by_activity_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 By Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 36\n",
    "_pairs = pairs.reset_index()\n",
    "ec50 = pd.DataFrame(_pairs[_pairs._DataQuality == 'ec50'].groupby(['mutated_Sequence']).count()['_MolID'])\n",
    "secondary = pd.DataFrame(_pairs[_pairs._DataQuality == 'secondaryScreening'].groupby(['mutated_Sequence']).count()['_MolID'])\n",
    "primary = pd.DataFrame(_pairs[_pairs._DataQuality == 'primaryScreening'].groupby(['mutated_Sequence']).count()['_MolID'])\n",
    "\n",
    "repartition = ec50.merge(secondary, left_index=True, right_index=True,how='outer').merge(primary, left_index=True, right_index=True,how='outer').replace(float('nan'),0)\n",
    "repartition['total'] = repartition['_MolID_x'] + repartition['_MolID_y'] + repartition['_MolID'] \n",
    "\n",
    "_repartition = repartition.sort_values(['total','_MolID','_MolID_y','_MolID_x'], ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(_repartition.index, height=_repartition._MolID_x, width=1.5)\n",
    "plt.bar(_repartition.index, bottom=_repartition._MolID_x, height=_repartition._MolID_y , width=1.5)\n",
    "plt.bar(_repartition.index, bottom=_repartition._MolID_x + _repartition._MolID_y, height=_repartition._MolID , width=1.5, color='silver')\n",
    "plt.xticks([],[])\n",
    "plt.legend(['EC50','Secondary','Primary'])\n",
    "plt.xlabel('Olfactory receptors', labelpad=25)\n",
    "plt.ylabel('Number of odorants tested', labelpad=25)\n",
    "plt.savefig(f'./Figures/Distribution_Odor_OR_by_test_{date.today()}.png',bbox_inches='tight',pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Combinatorial Code Full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 -1 Per Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_order = df_OR.sort_values('broadness', ascending=False).index\n",
    "_df = df_mols_joint.copy()\n",
    "_df = _df[_df['embedding_cluster_hdbscan'] >= 0]\n",
    "_temp=pd.DataFrame()\n",
    "_temp['100%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 1,)\n",
    "_temp['90%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.9,)\n",
    "_temp['75%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.75,)\n",
    "_temp['50%'] = _df.groupby('embedding_cluster_hdbscan').apply(get_quantile_per_odor_group, q = 0.5,)\n",
    "\n",
    "for clus in df_mols_joint.embedding_cluster_hdbscan.unique():\n",
    "    if clus >= 0: # Discarded unclustered molecules\n",
    "        cluster = clus\n",
    "        mat_cluster = df_predict_unstack.loc[df_mols_joint[df_mols_joint.embedding_cluster_hdbscan == cluster].sort_values('broadness', ascending=False).index, gene_order]\n",
    "        q50  = _temp.loc[cluster, '50%']\n",
    "        q75  = _temp.loc[cluster, '75%']\n",
    "        q90  = _temp.loc[cluster, '90%']\n",
    "        q100 = _temp.loc[cluster, '100%']\n",
    "        print('{} : {}'.format(clus, df_mols_joint[df_mols_joint.embedding_cluster_hdbscan == cluster].embedding_cluster_top_descriptor.unique()))\n",
    "        print(f\"Q(a)({0.5, 0.75, 0.9, 1.0}) = {q50},{q75},{q90},{q100}\")\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.matshow(mat_cluster)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        #plt.savefig(f'./Figures/Combinatorial_Code_Cluster_{cluster}_{date.today()}.png',bbox_inches='tight',pad_inches = 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2 Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_odor = df_odor.copy()\n",
    "_odor = _odor.set_index('InchiKey')\n",
    "_odor = _odor['GS_Odor type'].dropna()\n",
    "_odor = df_mols.join(_odor, how = 'inner')\n",
    "\n",
    "_odor_type = _odor.groupby('GS_Odor type').count()['predict']\n",
    "_odor_type = _odor_type # [_odor_type > 20]\n",
    "_odor = _odor[_odor['GS_Odor type'].isin(_odor_type.index)]\n",
    "\n",
    "df_odor_OR = None\n",
    "for smell in _odor_type.index:\n",
    "\n",
    "    _df = df_predict_unstack.loc[_odor[_odor['GS_Odor type'] == smell].index].copy()\n",
    "    _df = _df > 0.5\n",
    "    _tmp = _df.sum(axis = 0) / len(_df) \n",
    "    _tmp.name = 'active_rate'\n",
    "    _tmp.index.name = 'Gene'\n",
    "    _tmp = _tmp.to_frame().join(df_OR['broadness'], how = 'left')\n",
    "    _tmp['a/b'] = _tmp['active_rate'] / _tmp['broadness']\n",
    "    _tmp = pd.concat([_tmp], keys=[smell], names=['Odor'])\n",
    "    if df_odor_OR is None:\n",
    "        df_odor_OR = _tmp.copy()\n",
    "    else:\n",
    "        df_odor_OR = pd.concat([df_odor_OR,_tmp])\n",
    "\n",
    "_mols_order_by = 'embedding_cluster_top_descriptor' \n",
    "_gene_order_by = 'broadness' \n",
    "# --- Molecules ----------------------------------------------\n",
    "def get_ordered_mols(df, _mols_order_by, _df_predict_unstack, discard_unclustered = True):\n",
    "    _mols = df.loc[_df_predict_unstack.index.intersection(df.index)].copy()\n",
    "    _mols = _mols.dropna(subset = [_mols_order_by])\n",
    "    if _mols_order_by in ['embedding_cluster_hdbscan', 'embedding_cluster_top_descriptor']:\n",
    "        if discard_unclustered:\n",
    "            _mols = _mols[_mols['embedding_cluster_hdbscan'] >= 0]\n",
    "        if _mols_order_by == 'embedding_cluster_top_descriptor':\n",
    "            _mols['embedding_cluster_top_descriptor'] = _mols.apply(lambda x: str(x['embedding_cluster_top_descriptor']) + '_' + str(x['embedding_cluster_hdbscan']) , axis = 1)\n",
    "        _mols[_mols_order_by] = _mols[_mols_order_by].astype(str)\n",
    "\n",
    "        \n",
    "    _mols = _mols.sort_values([_mols_order_by, 'broadness'])\n",
    "    _mols = _mols[_mols_order_by]\n",
    "    return _mols\n",
    "\n",
    "# --- Receptors ----------------------------------------------\n",
    "def get_ordered_genes(df, _gene_order_by):\n",
    "    \"\"\"\n",
    "    df_OR\n",
    "    df_odor_OR\n",
    "    \"\"\"\n",
    "    if _gene_order_by == 'broadness':\n",
    "        _gene = df.copy()\n",
    "        _gene = _gene.sort_values('broadness')\n",
    "    elif _gene_order_by == 'family':\n",
    "        _gene = df.copy()\n",
    "        _gene['family'] = _gene.index.str.extract(pat = r'(\\d+)').values \n",
    "        _gene = _gene.sort_values(_gene_order_by)\n",
    "    elif _gene_order_by == 'family_broadness':\n",
    "        _gene = df.copy()\n",
    "        _gene['family'] = _gene.index.str.extract(pat = r'(\\d+)').values \n",
    "        _gene = _gene.sort_values(['family', 'broadness'])\n",
    "    else: #  _gene_order_by in df.index.get_level_values('Odor').unique():\n",
    "        _gene = df.copy()\n",
    "        _gene = _gene.loc[_gene_order_by, slice(None)].sort_values('a/b', ascending = False)    \n",
    "    return _gene\n",
    "\n",
    "\n",
    "_mols = get_ordered_mols(df_mols_joint, _mols_order_by, df_predict_unstack)\n",
    "if _gene_order_by in df_odor_OR.index.get_level_values('Odor').unique():\n",
    "    _gene = get_ordered_genes(df_odor_OR, _gene_order_by)\n",
    "else:\n",
    "    _gene = get_ordered_genes(df_OR, _gene_order_by)\n",
    "_gene_idx = _gene.index\n",
    "# --- Plot ----------------------------------------------\n",
    "def _tmp_func(x, _discard_idx):\n",
    "    if x in _discard_idx:\n",
    "        return ''\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "_tmp = _mols.reset_index().groupby(by = _mols_order_by, sort = True).count()['InchiKey']\n",
    "_xticks = _tmp.cumsum() - 1\n",
    "_xticks = _xticks.reset_index()\n",
    "_discard_idx = _tmp[_tmp < 10].index\n",
    "_xticks[_mols_order_by] = _xticks[_mols_order_by].apply(lambda x: _tmp_func(x, _discard_idx))\n",
    "del _tmp, _discard_idx\n",
    "_tmp = df_predict_unstack.loc[_mols.index][_gene_idx]\n",
    "\n",
    "plt.rcParams['font.size']=2\n",
    "\n",
    "fig = plt.figure(dpi = 300, figsize = [15, 10])\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(_tmp.T.values)\n",
    "\n",
    "ax.set_xticks(list(_xticks['InchiKey']), list(_xticks[_mols_order_by]), fontsize=2, rotation=90) # fontsize = 3\n",
    "ax.xaxis.set_tick_params(width=0.1)\n",
    "\n",
    "if _gene_order_by in ['family', 'family_broadness']:\n",
    "    _tmp = _gene.reset_index().groupby(by = 'family', sort = True).count()['Gene']\n",
    "    _yticks = _tmp.cumsum() - 1\n",
    "    _yticks = _yticks.reset_index()\n",
    "    ax.set_yticklabels(list(_yticks['Gene']), list(_yticks['family']), fontsize = 2) # fontsize = 2\n",
    "    ax.yaxis.set_tick_params(width=0.1, rotation = 90)\n",
    "    ax.set_yticks(list(range(len(_gene_idx))),_gene_idx.to_list(), fontsize=2)\n",
    "    ax.yaxis.set_tick_params(width=0.1)\n",
    "    del _tmp\n",
    "else:\n",
    "    ax.set_yticks(list(range(len(_gene_idx))),_gene_idx.to_list())\n",
    "    ax.set_yticks([],[])\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('test_figures')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d445a9a248f219919bbe242c8e8c47c817dc016453eea268239d9fb0b1562f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
